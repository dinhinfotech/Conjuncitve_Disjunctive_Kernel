% !TeX root = link_prediction_for_diffusion_kernels.tex

\section{Notation and Background}
\label{background}
Let us consider an undirected graph $G = (V, E)$ in which $V$ represents  a set of entities (vertices)  and $E$ characterizes the entity relationships (links). The adjacency matrix $A$ is a symmetric matrix used to describe the direct links between vertices $v_{i}$ and $v_{j}$ in the graph. Any entry $A_{ij}$ is equal to 1 when there exists a link connecting $v_{i}$ and $v_{j}$, and is 0 otherwise. The Laplacian matrix $L$ is defined as $L = D-A$, where $D$ is the diagonal matrix with non-null entries equal to the summation over the corresponding row of the adjacency matrix, i.e. $D_{ii}=\sum_j A_{ij}$. %The rest of the paper are described under this notation convention. 
\subsubsection{Graph Node Kernels.}
%As the desire of having a good node similarity measure for building graph-based leanring systems, many graph node kernels have been introduced and applied. 
A graph node kernel is a kernel which defines the similarity between nodes in a graph. Formally, a graph node kernel, $k(\cdot,\cdot)$, is defined as $k: V \times V \longrightarrow \mathbb{R}$ such that $k$ is symmetric positive semidefinite. Most graph node kernels belong to one of  two popular frameworks: diffusion-based graph node kernels and decomposition graph node kernels. 

Diffusion-based kernels can be considered as modifications of the laplacian diffusion kernel \cite{proceeding2}. These kernels measure the node proximity between any couple of nodes by taking into account the paths connecting them. They normally show state-of-the-art performance when dealing with dense graphs because of their ability to capture a global similarity measure. However, they perform poorly  when facing  sparse graphs with a low number of links and a high number of disconnected components. In the following, we briefly describe some of the most popular diffusion-based graph node kernels.
\begin{itemize}
\item \textit{Laplacian exponential diffusion kernel:} One of the most well-known kernels for graphs is the Laplacian exponential diffusion kernel (LEDK), as it is widely used for exploiting discrete structures in general and graphs in particular. On the basis of the heat diffusion dynamics, Kondor and Lafferty proposed LEDK in \cite{proceeding2}: imagine to initialize each vertex with a given amount of heat and let it flow through the edges until an arbitrary instant of time. The similarity between any vertex couple $v_{i}$, $v_{j}$ is the amount of heat starting from $v_{i}$ and reaching $v_{j}$ within the given time. Therefore, LEDK can capture the long range relationship between vertices of a graph to define the global similarities. The formula to compute the LEDK kernel matrix is:
\begin{equation}
K_{LEDK} = e^{-\beta L}\; ,
\end{equation}
where $\beta$ is the diffusion parameter used to control the rate of diffusion, and $e^{X}=\sum_{k=0}^{\infty} \frac{1}{k!}X^k$ refers to the matrix exponential for matrix $X$. Choosing a consistent value for $\beta$ is very important: on the one side, if $\beta$ is too small, the local information cannot be diffused effectively and, on the other side, if it is too large, the local information will be lost. $K_{LEDK}$ is positive semi-definite as proved in \cite{proceeding2}.

\item \textit{Markov exponential diffusion kernel:} In LEDK, similarity values between high degree vertices is generally higher compared to that between low degree ones. Intuitively, the more paths connect two vertices, the more heat can flow between them. This could be problematic since peripheral nodes have unbalanced similarities with respect to central nodes. In order to make the strength of individual vertices comparable, a modified version of LEDK is introduced by Chen et al in \cite{proceeding3}.  This kernel is dubbed Markov exponential diffusion kernel (MEDK) and its kernel matrix is obtained by the following formula:
\begin{equation}
K_{MEDK} = e^{-\beta M}\; .
\end{equation}
The difference with respect to the LEDK is the replacement of \textit{L} by the matrix $M = (D-A-nI)/n$ where \textit{n} is the total number of vertices in graph and I is the identity matrix. The role of $\beta$ is the same as for LEDK.

\item \textit{Markov diffusion kernel:} The original Markov diffusion kernel MDK is introduced by Fouss et al. \cite{jour3}. It exploits the idea of diffusion distance, which is a measure of how similar the pattern of heat diffusion is between a pair of initialized nodes. In other words, it expresses how much nodes ``influence'' each other in a similar fashion. If their diffusion ways are alike, the similarity will be high and, vice versa, it will be low if they diffuse differently. The corresponding kernel matrix is computed starting from the transition matrix \textit{P} ($P = D^{-1} A$) and by defining $Z(t) = \frac{1}{t}\sum_{\tau=1}^{t} P^{\tau}$, as follows:
\begin{equation}
K_{MDK} = Z(t) Z^{\top}(t)\; .
\end{equation}

\item \textit{Regularized Laplacian kernel:} Another popular graph node kernel function used in graph mining is the regularized Laplacian kernel (RLK). This kernel function was introduced by Chebotarev and Shamis in \cite{proceeding4} and represents a normalized version of the random walk with restart model. Its kernel matrix is defined as follows:
\begin{equation}
K_{RLK} = \sum_{n=0}^{\infty}\beta^{n}(-L)^n\; ,
\end{equation}
where the parameter $\beta$ is again the diffusion parameter. RLK counts the paths connecting two nodes on the graph induced by taking \textit{-L} as the adjacency matrix, regardless of the path length. Thus, a non-zero value is assigned to any couple of nodes as long as they are connected by any indirect path. $K_{RLK}$ remains a relatedness measure even when the diffusion factor is large, by virtue of the negative weights assigned to self-loops.
\end{itemize}
Decomposition graph node kernels take the idea from \cite{proceeding5} in which the similarity function between two graphs can be formed by decomposing each graph into subgraphs and by devising a valid local kernel between the subgraphs. This idea is then adjusted to measure graph node similarity by considering the neighborhood subgraph rooted at a vertex as its graph to compute. In order to form this kind of kernel,  the graph matching problem, or equivalently the graph isomorphic problem, need to be solved, which is not known to be solvable in polynomial time nor to belong to the NP-complete complexity class. An advantage of using decomposition kernels is the possibility to have non-zero similarity values for node couples located in distinct disconnected components of a graph. A recent and effective decomposition graph node kernel is the Conjunctive and Disjunctive Node Kernel (CDNK), proposed in \cite{proceeding6}. CDNK is an extension of NSPDK \cite{proceeding7}, which is an instance of convolution kernel (decomposition kernel). Considering a couple of nodes $u$ and $v$, the CDNK kernel defines the similarity between them by taking into account the common pairwise neighborhood subgraphs rooted at $u$ and $v$.
\subsubsection{Link Enrichment.}
\label{link-enrichment}
Link enrichment is a task that intends to add the most likely non-observed links into a graph. This task can be performed by first using a link prediction method to make a ranking over all non-observed links based on their probabilities to be actual links, and then the top non-observed links are added into the graph. A considerable number of link prediction methods have been proposed in the literature. These methods can be classified into different categories as discussed in \cite{jour2}: \textit{similarity-based algorithms}, \textit{maximum likelihood methods}, and \textit{probabilistic models}. Similarity-based methods assign for each non-observed link a score and this score is then directly used as the proximity between starting and ending nodes of that link. In maximum likelihood methods, some organizing principles of the network structure are assumed. Then, the likelihood of any non-observed link can be calculated according to corresponding rules and parameters. Probabilistic models aim at abstracting the underlying structure from the observed network, and to predict the missing links by using a learned model. Given a target graph G, the probabilistic model will optimize a built target function to establish a model composed of a group of parameters which can best fit the observed data of the target network.

In this paper, we employ five graph node kernels described in the previous section: LEDK, MEDK, MDK, RLK and CDNK for link prediction since they belong to global similarity-based group. There are two reasons for the use of global similarity-based methods. Firstly, among similarity-based algorithms, global ones show, in general, better results   than local and semi-local similarity-based algorithms. Secondly, similarity-based algorithms are much simpler to deal with (and computationally less demanding) than maximum likelihood methods and probabilistic models.