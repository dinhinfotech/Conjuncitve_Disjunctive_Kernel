%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2017 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2017,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2017} with
% \usepackage[nohyperref]{icml2017} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2017} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2017}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Submission and Formatting Instructions for ICML 2017}

\begin{document} 

\twocolumn[
\icmltitle{Link Enrichment to Strengthen Diffusion-based Graph Node Kernels}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2017
% package.

% list of affiliations. the first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% you can specify symbols, otherwise they are numbered in order
% ideally, you should not use this facility. affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Dinh Tran Van}{to}
\icmlauthor{Alessandro Sperduti}{to}
\icmlauthor{Fabrizio Costa}{goo}
\end{icmlauthorlist}


\icmlaffiliation{to}{Department of Mathematics, Padova University, Trieste, 63, 35121 Padova, Italy}
\icmlaffiliation{goo}{Department of Computer Science, University of Exeter Exeter EX4 4QF, UK}

\icmlcorrespondingauthor{Dinh Tran Van}{dinh@math.unipd.it}
\icmlcorrespondingauthor{Fabrizio Costa}{f.costa@exeter.ac.uk}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Link Enrichment, Graph Node Kernels}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
%\footnotetext{hi}

\begin{abstract}
Many predictive tasks on networks assume complete information on the network topology. In many real world cases however, the available relational information is incomplete. In these cases one should solve the problem of missing links prediction prior to the predictive task of interest.
In this paper we study a class of predictive methods known as  diffusion-based kernels. We propose a link enrichment method to improve upon these approaches based on a flexible graph kernel that can exploit heterogeneous sets of information sources. 
\end{abstract} 

\section{Introduction}
\label{introduction}
Recently, with the fast development of science and technology, we have witnessed the rapid growth of data in terms of volume and variety. In order to efficiently extract knowledge from those huge data, a number of learning systems have been introduced. Each system takes specific types to represent data. Graph-based learning systems are learning systems which consider graphs as their input and they are widely used in different domains \cite{references here}.

In graph-based systems, the measurement of node proximity is one of the key factor that determines the performance of the systems. The most common paradigm used to capture node similarity is using graph node kernels. Graph node kernel is a paradigm which allows to define similarity between any couple of graph nodes in a normally high dimensional space. As a consequence, considerable graph node kernels have been proposed and applied in many application, domains. Among them, diffusion-based kernel \footnote{A diffusion-based graph node kernel measures the proximity between a couple of nodes by taking into account paths connecting them.} are the most commonly employed and show promising results. However, those node kernels usually show good performance when dealing with dense graphs - graphs with high value of an average high node degree node. And, vice versa, they usually lead to poor performance when working with sparse graphs.

Data collected from domains are normally in complete. This raises challenge for graph-based learning systems. One of the reasons to explain for that is the lack of links connecting nodes. To overcome this problem, a potential solution is to use link enrichment - link enrichment is a task that aims at recovering the missing links (or future links in evolution networks). There exist many link enrichment methods and they can be classified into different groups \cite{ref-here}. The simplest and most widely used framework is the similarity-based algorithms where each pair of nodes is assigned a score which is directly defined as the similarity between nodes.

To the best of our knowledge, there is no investigation that has been done to boost the performance of diffusion-based kernels by using link enrichment. Therefore, in this paper, we present a method that intends to strengthen the power of diffusion-based graph node kernels by employing link enrichment paradigm. The experimental results confirm that our proposed method is notable when using diffusion-based graph node kernels.

This paper is organized as follows: we first introduce the notation and background in the section \ref{background}. We then describe our proposed method in section \ref{method}. The evaluation and results are presented in section \ref{evaluation} and section \ref{results-discussion}, respectively. Finally, the conclusion is writen in section \ref{conclusion}.

\section{Notation and Background}
\label{background}
Let us consider an undirected, weighted graph $G = (V, E)$ representing entities and relationships among them. The adjacency matrix $\textbf{A}$ is a symmetric matrix used to characterize the direct links between vertices $v_{i}$ and $v_{j}$ in the graph. Any entry $A_{ij}$ is equal to $w_{ij}$ when there exists an edge of weight $w_{ij}>0$ connecting $v_{i}$ and $v_{j}$, and is 0 otherwise. The Laplacian matrix $\textbf{L}$ is defined as $\textbf{L} = \textbf{D}-\textbf{A}$, where $\textbf{D}$ is the diagonal matrix with non-null entries equal to the summation over the corresponding row of the adjacency matrix, i.e. $D_{ii}=\sum_j A_{ij}$. The following graph node kernels are described under this notation convention. 
\subsection{Diffusion-based Graph Node Kernels}
%------------------------
\textit{Laplacian exponential diffusion kernel} \newline
One of the most well-known kernels for graphs is the Laplacian exponential diffusion kernel $\textbf{K}_{LED}$, as it is widely used for exploiting discrete structures in general and graphs in particular. On the basis of the heat diffusion dynamics, Kondor and Lafferty proposed $\textbf{K}_{LED}$ in \cite{ledk}: imagine to initialize each vertex with a given amount of heat and let it flow through the edges until an arbitrary instant of time. The similarity between any vertex couple $v_{i}$, $v_{j}$ is the amount of heat starting from $v_{i}$ and reaching $v_{j}$ within the given time. Therefore, $\textbf{K}_{LED}$ can capture the long range relationship between vertices of a graph to define the global similarities. Below is the formula to compute $\textbf{K}_{LED}$ values:
\begin{equation} \label{LEDK-formula}
\textbf{K}_{LED} = e^{-\beta \textbf{L}} = \textbf{I} - \beta \textbf{L} + \frac{\beta \textbf{L}^{2}}{2!} - ...
\end{equation}
where $\beta$ is the diffusion parameter and is used to control the rate of diffusion and $\textbf{I}$ is the identity matrix. Choosing a consistent value for $\beta$ is very important: on the one side, if $\beta$ is too small, the local information cannot be diffused effectively and, on the other side, if it is too large, the local information will be lost. $\textbf{K}_{LED}$ is positive semi-definite as proved in \cite{ledk}. \newline \newline
%--------------------
%--------------------
\textit{Markov exponential diffusion kernel} \newline
In $\textbf{K}_{LED}$, similarity values between high degree vertices is generally higher compared to that between low degree ones. Intuitively, the more paths connect two vertices, the more heat can flow between them. This could be problematic since peripheral nodes have unbalanced similarities with respect to central nodes. In order to make the strength of individual vertices comparable, a modified version of $\textbf{K}_{LED}$ was introduced by Chen et al in \cite{mrf}, called Markov exponential diffusion kernel $\textbf{K}_{MED}$ and given by the following formula:
\begin{equation} \label{MEDK-formula}
\textbf{K}_{MED} = e^{-\beta \textbf{M}}
\end{equation}
The difference with respect to the Laplacian diffusion kernel is the replacement of $\textbf{L}$ by the matrix $\textbf{M}=(\textbf{D}-\textbf{A}-n\textbf{I})/n$ where $n$ is the total number of vertices in graph. The role of $\beta$ is the same as for $\textbf{K}_{LED}$. \newline \newline
%-------------------
%-------------------
\textit{Markov diffusion kernel} \newline
The original Markov diffusion kernel $\textbf{K}_{MD}$ was introduced by Fouss et al. \cite{mdk} and exploits the idea of diffusion distance, which is a measure of how similar the pattern of heat diffusion is among a pair of initialized nodes. In other words, it expresses how much nodes "influence" each other in a similar fashion. If their diffusion ways are alike, the similarity will be high and, vice versa, it will be low if they diffuse differently. This kernel is computed starting from the transition matrix $\textbf{P}$ and by defining $\textbf{Z}(t) = \frac{1}{t}\sum_{\tau=1}^{t}\textbf{P}^{\tau}$, as follows:
\begin{equation} \label{MDK-formula}
\textbf{K}_{MD} = \textbf{Z}(t)\textbf{Z}^{\top}(t)
\end{equation}
\newline \newline
%-------------------
%-------------------
\textit{Regularized Laplacian kernel} \newline
Another popular graph node kernel function used in graph mining is the regularized Laplacian kernel $\textbf{K}_{RL}$. This kernel function was introduced by Chebotarev and Shamis in \cite{rlk} and represents a normalized version of the random walk with restart model. It is defined as follows:
\begin{equation} \label{RLK-formula}
\textbf{K}_{RL} = \sum_{n=0}^{\infty}\beta^{n}(-\textbf{L})^n = (\textbf{I} + \beta \textbf{L})^{-1}
\end{equation}
where the parameter $\beta$ is again the diffusion parameter. $\textbf{K}_{RL}$ counts the paths connecting two nodes on the graph induced by taking $-\textbf{L}$ as the adjacency matrix, regardless of the path length. Thus, a non-zero value is assigned to any couple of nodes as long as they are connected by any indirect path. $\textbf{K}_{RL}$ remains a relatedness measure even when diffusion factor is large, by virtue of the negative weights assigned to self-loops.


\subsection{Link Enrichment}
\label{link-enrichment}
Graphs are widely used to represent data in different domains. Since information encoded in data normally are not complete, it impacts on the performance of graph-based learning systems. Link enrichment (link prediction) is a task that intends to add the most likely non-observed links into graphs. There is a number of link prediction methods which have been proposed. These methods can be classified into different categories as presented in \cite{mitchell80}. The simpliest framwork for link prediction is Similarity-Based Algorithms where each pair of nodes is assigned a score which is directly defined as the similarity. In this paper, we employ global similarity methods which belong to Similarity-Based Algorithms to predict the most likely of non-observed links to add into graphs.

\section{Method}
\label{method}
In this section, we present the link enrichment method for strengthening diffusion-based graph node kernels.

Given a graph $G=(V, E)$, our method consists of two pharses:
\begin{itemize}
\item Link enrichment: in the first phase, Starting from the graph $G$, we apply one of the link prediction method on $G$ to compute a score for each of non-observed link. This score represents for its probability to be considered as a link in the graph. The non-observed link list is then sorted by their corresponding scores. The top links in the sorted list are added into $G$ to have new graph $G^{'}$.
\item Kernel computation: in the second phase, we apply diffusion-based graph node kernels to the graph $G^{'}$ to compute kernel matrix which encodes the similarities between any couple of nodes. This 
\end{itemize}
\section{Evaluation}
\label{evaluation}
We perform an empirical evaluation of the predictive performance of several kernel based methods on two of the databases used in \cite{medk}.

\textbf{BioGPS:} a gene co-expression network is constructed from BioGPS dataset, which contains 79 tissues, measured with the Affymetrix U133A array. Edges are inserted when the pairwise Pearson correlation coefficient (PCC) between genes is larger than 0.5.

\textbf{HPRD:} a database of curated proteomic information pertaining to human proteins. It is derived from \cite{hprd} with 9465 vertices and 37039 edges. We enploy the HPRD version used in \cite{medk} in which they remove some vertices to have 7311 vertices at the end.

To evaluate the performance of graph node kernels we analyze the {\em gene prioritization}, i.e. given a set of genes known to be associated to a given disease, gene prioritization is the task to rank the candidate genes based on their probabilities to be related to that disease. Similar to the evaluation process used in \cite{medk}, we choose 12 diseases with at least 30 confirmed genes. For each disease, we construct a positive set $\mathcal{P}$ with all confirmed disease genes, and a negative set $\mathcal{N}$ which contains random genes associated at least to one disease class which is not related to the class that is defining the positive set. In \cite{medk} the ratio between the dataset sizes is chosen as $\vert \mathcal{N} \vert =
\frac{1}{2} \vert \mathcal{P} \vert$. The predictive performance of each method is evaluated via a leave-one-out cross validation: one gene is kept out in turn and the rest are used to train an SVM model. We compute a decision score $q_i$ for the test gene $g_i$ as the top percentage value of score $s_i$ among all candidate gene scores. We collect all decision scores for every gene in the training set to form a global decision score list on which we compute the AUC ROC.

\textbf{Model Selection.}
The hyper parameters of the various methods are set using a k-fold on a dataset set that is then never used in the predictive performance estimate. We try the values for diffusion parameter in DK and MED in $\lbrace 10^{-3}, 10^{-3}, 10^{-2}, 10^{-1} \rbrace$, time steps in MD in $\lbrace 1, 10, 100 \rbrace$ and RL parameter in $\lbrace 1, 4, 7 \rbrace$. For CDNK, we try for the degree threshold values in $\lbrace 10,\ 15,\ 20 \rbrace$, clique size threshold in $\lbrace 4,\ 5 \rbrace$, maximum radius in $\lbrace 1,\ 2 \rbrace$, maximum distance in $\lbrace 2,\ 3,\ 4 \rbrace$. Finally, the $C$ of SVM is searched in $\lbrace 10^{-5},  \ 10^{-4}, \ 10^{-3},\ 10^{-2},\ 10^{-1}, 1,\ 10,\ 10^2 \rbrace$.
\section{Results and Discussion}
\label{results-discussion}
\begin{table*}[b]
%\vspace*{-5pt}
\centering
%\setlength{\tabcolsep}{0.7mm}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
         \multicolumn{9}{|c|}{\textbf{HPRD Dataset}}\\
 \hline
 & \multicolumn{2}{c|}{\textbf{LEDK}} & \multicolumn{2}{c|}{\textbf{MEDK}} & \multicolumn{2}{c|}{\textbf{MDK}} & \multicolumn{2}{c|}{\textbf{RLK}} \\
 \hline
  Disease & No\_Link & Link & No\_Link & Link & No\_Link & Link & No\_Link & Link \\
 \hline
  0 & 73/2 & 74/1 & 71/1 & 71/2 & 71/1 & 69/2 & 73/2 & 73/1 \\

  1 & 68/2 & 69/1 & 54/2 & 55/1 & 82/2 & 82/1 & 80/2 & 80/1 \\

  2 & 77/1 & 75/2 & 72/1 & 70/2 & 79/1 & 77/2 & 81/1 & 76/2 \\

  3 & 60/2 & 62/1 & 62/2 & 63/1 & 63/2 & 66/1 & 65/2 & 67/1 \\

  4 & 67/2 & 68/1 & 65/2 & 67/1 & 68/2 & 70/1 & 68/1 & 68/2 \\

  5 & 67/1 & 67/2 & 69/1 & 68/2 & 65/2 & 68/1 & 66/2 & 70/1 \\

  6 & 87/2 & 88/1 & 87/2 & 87/1 & 87/2 & 89/1 & 87/2 & 88/1 \\

  7 & 79/2 & 80/1 & 78/2 & 79/1 & 81/1 & 78/2 & 79/1 & 77/2 \\

  8 & 78/2 & 79/1 & 72/2 & 72/1 & 79/1 & 75/2 & 80/2 & 80/1 \\

  9 & 73/2 & 76/1 & 71/2 & 73/1 & 70/2 & 72/1 & 70/2 & 77/1 \\

  10 & 80/1 & 79/2 & 82/1 & 81/2 & 77/1 & 75/2 & 80/2 & 80/1 \\

  11 & 76/1 & 75/2 & 75/2 & 75/1 & 83/2 & 87/1 & 84/1 & 83/2 \\

 \hline
  $\overline{AUC}$ & 74/1.67 & 74/1.33 & 71/1.67 & 72/1.33 & 75/1.58 & 76/1.42 & 76/1.67 & 77/1.33 \\
\hline
    & & 0.76 & & 0.73 & & 0.76 & & 0.77 \\
\hline
\end{tabular}
\caption{\textit {Predictive performance on 12 gene-disease associations using network induced by the HPRD. We report the AUC-ROC (\%) and the rank for each kernel method.}}
\label{table:results}
\end{table*}
%==============================================
\begin{table*}[b]
%\vspace*{-5pt}
\centering
%\setlength{\tabcolsep}{0.7mm}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
         \multicolumn{9}{|c|}{\textbf{HPRD Dataset}}\\
 \hline
 & \multicolumn{2}{c|}{\textbf{LEDK}} & \multicolumn{2}{c|}{\textbf{MEDK}} & \multicolumn{2}{c|}{\textbf{MDK}} & \multicolumn{2}{c|}{\textbf{RLK}} \\
 \hline
  Disease & No\_Link & Link & No\_Link & Link & No\_Link & Link & No\_Link & Link \\
 \hline
	0 & 73/2 & 76/1 & 76/2 & 79/1 & 70/1 & 69/2 & 70/1 & 67/2 \\

	1 & 60/1 & 61/2 & 59/1 & 58/2 & 60/2 & 61/1 & 60/2 & 72/1 \\

	2 & 85/1 & 84/2 & 85/1 & 84/2 & 82/1 & 82/2 & 82/2 & 84/1 \\

	3 & 66/2 & 67/1 & 56/2 & 60/1 & 66/2 & 69/1 & 66/2 & 67/1 \\

	4 & 61/2 & 62/1 & 63/1 & 62/2 & 58/2 & 58/1 & 58/2 & 58/1 \\

	5 & 70/1 & 69/2 & 70/1 & 69/2 & 67/2 & 68/1 & 67/2 & 68/1 \\

	6 & 73/1 & 71/2 & 68/1 & 68/2 & 69/1 & 69/2 & 69/2 & 76/1 \\

	7 & 69/2 & 69/1 & 69/2 & 69/1 & 67/2 & 67/1 & 67/1 & 65/2 \\

	8 & 73/2 & 74/1 & 70/2 & 71/1 & 65/2 & 68/1 & 65/2 & 67/1 \\

	9 & 69/1 & 67/2 & 65/2 & 66/1 & 64/2 & 65/1 & 64/1 & 64/2 \\

	10 & 58/2 & 60/1 & 52/2 & 56/1 & 60/2 & 61/1 & 60/2 & 70/1 \\

	11 & 69/2 & 73/1 & 70/2 & 70/1 & 60/2 & 61/1 & 60/2 & 62/1 \\
	
	\hline
	\shortstack{$\overline{AUC}$ \\$\overline{Rank}$} & 69/1.58 & 69/1.42 & 67/1.58 & 68/1.42 & 66/1.75 & 67/1.25 & 66/1.75 & 68/1.25 \\
\hline
    & & 72 & & 69 & & 68 & & 71 \\      						
\hline
\end{tabular}
\caption{\textit {Predictive performance on 12 gene-disease associations using network induced by the BioGPS. We report the AUC-ROC (\%) and the rank for each kernel method.}}
\label{table:results}
\end{table*}

\section{Conclusion}
\label{conclusion}
In this paper, we have proposed a new novel method to strengthen the boost the power of diffusion-based graph node kernel by using link enrichment paradigm.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2017}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
